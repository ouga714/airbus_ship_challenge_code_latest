#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""


Airbus Ship Detection (intern-winter / momos) — Ablation Condition ②:
- Loss: BCE + Tversky-loss (soft Tversky)
    Tversky = TP / (TP + alpha*FP + beta*FN)
    Default: alpha=0.6, beta=0.4  (precision-weighted, matches recall:precision=4:6 mapping used in your spec)
- Best epoch selection: maximize VAL soft-Tversky (threshold-free)
- Threshold tuning: maximize pixel-level F2 on VAL subset (TTA mandatory)
- Early stopping: patience, max_epochs=50
- Inference:
    * Pseudo-Test (from train_v2, disjoint) -> metrics: Precision/Recall/F1/F2/IoU (TTA mandatory)
    * Kaggle test_v2 -> variable-row submission (instance-per-row RLE; ImageId duplicates allowed; empty => one blank row)
- Outputs (under out_dir/run_name/):
    * submission.csv
    * run_global.log
    * run_detail.log (iteration + NaN diagnostics)
    * summary.json
- RLE: Fortran order (column-major). Round-trip self-test included.
- No AMP (FP32)

Path handling:
- Intern-winter: pass --data_dir and --out_dir (defaults are set for typical /workspace layout)
Example:
python3 intern_winter_ablation_bce_tversky_tta_es.py \
  --data_dir /workspace/kaggle_competition/airbus-ship-detection \
  --out_dir  /workspace/kaggle_competition/outputs_airbus_ablation_tversky \
  --seed 42
"""

import os
import sys
import csv
import json
import time
import math
import random
import hashlib
import argparse
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import torchvision

_HAS_CV2 = False
try:
    import cv2
    _HAS_CV2 = True
except Exception:
    _HAS_CV2 = False

_HAS_SCIPY = False
try:
    import scipy.ndimage as ndi
    _HAS_SCIPY = True
except Exception:
    _HAS_SCIPY = False


# -------------------------
# Logging
# -------------------------
class DualLogger:
    def __init__(self, path: str, also_stdout: bool):
        self.path = path
        self.also_stdout = also_stdout
        d = os.path.dirname(path)
        if d:
            os.makedirs(d, exist_ok=True)
        self.f = open(path, "w", encoding="utf-8")

    def log(self, msg: str):
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        line = f"{ts} {msg}"
        self.f.write(line + "\n")
        self.f.flush()
        if self.also_stdout:
            print(line)

    def close(self):
        try:
            self.f.close()
        except Exception:
            pass


# -------------------------
# Reproducibility
# -------------------------
def seed_everything(seed: int):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# -------------------------
# RLE (Fortran order) + self-test
# -------------------------
def rle_encode_fortran(mask: np.ndarray) -> str:
    if mask.ndim != 2:
        raise ValueError(f"mask must be 2D, got {mask.shape}")
    m = (mask > 0).astype(np.uint8)
    flat = m.flatten(order="F")
    if flat.sum() == 0:
        return ""
    flat = np.concatenate([[0], flat, [0]])
    runs = np.where(flat[1:] != flat[:-1])[0] + 1
    runs[1::2] -= runs[0::2]
    return " ".join(map(str, runs.tolist()))

def rle_decode_fortran(rle: str, shape: Tuple[int, int]) -> np.ndarray:
    H, W = shape
    if rle is None:
        return np.zeros((H, W), dtype=np.uint8)
    rle = str(rle).strip()
    if rle == "" or rle.lower() == "nan":
        return np.zeros((H, W), dtype=np.uint8)
    s = list(map(int, rle.split()))
    starts = np.asarray(s[0::2], dtype=np.int64) - 1
    lengths = np.asarray(s[1::2], dtype=np.int64)
    ends = starts + lengths
    flat = np.zeros(H * W, dtype=np.uint8)
    for st, en in zip(starts, ends):
        flat[st:en] = 1
    return flat.reshape((H, W), order="F")

def rle_roundtrip_selftest():
    rng = np.random.default_rng(0)
    shape = (64, 64)
    for _ in range(200):
        m = (rng.random(shape) < 0.05).astype(np.uint8)
        r = rle_encode_fortran(m)
        d = rle_decode_fortran(r, shape)
        if not np.array_equal(m, d):
            raise AssertionError("RLE round-trip failed (Fortran order)")
    z = np.zeros(shape, dtype=np.uint8)
    assert rle_encode_fortran(z) == ""
    assert rle_decode_fortran("", shape).sum() == 0


# -------------------------
# CSV: ImageId -> list[RLE]
# -------------------------
def load_id2rles(csv_path: str) -> Dict[str, List[str]]:
    id2rles: Dict[str, List[str]] = {}
    with open(csv_path, "r", newline="") as f:
        reader = csv.DictReader(f)
        if "ImageId" not in reader.fieldnames or "EncodedPixels" not in reader.fieldnames:
            raise RuntimeError(f"Unexpected CSV header: {reader.fieldnames}")
        for row in reader:
            img_id = row["ImageId"]
            rle = row["EncodedPixels"]
            if rle is None:
                continue
            rle = str(rle).strip()
            if rle == "" or rle.lower() == "nan":
                continue
            id2rles.setdefault(img_id, []).append(rle)
    return id2rles

def list_image_ids(img_dir: str) -> List[str]:
    exts = (".jpg", ".jpeg", ".png")
    ids = [fn for fn in os.listdir(img_dir) if fn.lower().endswith(exts)]
    ids.sort()
    if len(ids) == 0:
        raise RuntimeError(f"No images found in: {img_dir}")
    return ids


# -------------------------
# Split (stratified, disjoint)
# -------------------------
@dataclass
class SplitConfig:
    seed: int = 42
    train_n: int = 20000
    val_n: int = 3000
    pseudo_test_n: int = 3000
    pos_frac: float = 0.40

def stratified_disjoint_split(all_ids: List[str], pos_set: set, cfg: SplitConfig) -> Dict[str, List[str]]:
    rng = np.random.default_rng(cfg.seed)
    pos_ids = [i for i in all_ids if i in pos_set]
    neg_ids = [i for i in all_ids if i not in pos_set]
    rng.shuffle(pos_ids)
    rng.shuffle(neg_ids)

    def take_counts(total_n: int) -> Tuple[int,int]:
        pos_n = int(round(total_n * cfg.pos_frac))
        neg_n = total_n - pos_n
        return pos_n, neg_n

    tr_pos, tr_neg = take_counts(cfg.train_n)
    va_pos, va_neg = take_counts(cfg.val_n)
    te_pos, te_neg = take_counts(cfg.pseudo_test_n)

    need_pos = tr_pos + va_pos + te_pos
    need_neg = tr_neg + va_neg + te_neg
    if need_pos > len(pos_ids):
        raise RuntimeError(f"Not enough POS images: need {need_pos}, have {len(pos_ids)}")
    if need_neg > len(neg_ids):
        raise RuntimeError(f"Not enough NEG images: need {need_neg}, have {len(neg_ids)}")

    train = pos_ids[:tr_pos] + neg_ids[:tr_neg]
    val = pos_ids[tr_pos:tr_pos+va_pos] + neg_ids[tr_neg:tr_neg+va_neg]
    test = pos_ids[tr_pos+va_pos:tr_pos+va_pos+te_pos] + neg_ids[tr_neg+va_neg:tr_neg+va_neg+te_neg]

    rng.shuffle(train); rng.shuffle(val); rng.shuffle(test)
    return {"train": train, "val": val, "pseudo_test": test}


# -------------------------
# Image IO + normalization
# -------------------------
_IMNET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)[:, None, None]
_IMNET_STD  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)[:, None, None]

def normalize_imagenet(x: torch.Tensor) -> torch.Tensor:
    return (x - _IMNET_MEAN.to(x.device)) / _IMNET_STD.to(x.device)

def read_rgb(path: str) -> np.ndarray:
    if _HAS_CV2:
        bgr = cv2.imread(path, cv2.IMREAD_COLOR)
        if bgr is None:
            raise RuntimeError(f"Failed to read image: {path}")
        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
    else:
        from PIL import Image
        return np.array(Image.open(path).convert("RGB"))

def resize_rgb(rgb: np.ndarray, size: int) -> np.ndarray:
    if rgb.shape[0] == size and rgb.shape[1] == size:
        return rgb
    if _HAS_CV2:
        return cv2.resize(rgb, (size, size), interpolation=cv2.INTER_LINEAR)
    from PIL import Image
    return np.array(Image.fromarray(rgb).resize((size, size)))

def resize_mask(mask: np.ndarray, size: int) -> np.ndarray:
    if mask.shape[0] == size and mask.shape[1] == size:
        return mask
    if _HAS_CV2:
        return cv2.resize(mask, (size, size), interpolation=cv2.INTER_NEAREST)
    from PIL import Image
    return np.array(Image.fromarray(mask).resize((size, size), resample=Image.NEAREST))


class AirbusTrainDataset(Dataset):
    def __init__(self, img_dir: str, image_ids: List[str], id2rles: Dict[str, List[str]], size: int):
        self.img_dir = img_dir
        self.ids = image_ids
        self.id2rles = id2rles
        self.size = size

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx: int):
        img_id = self.ids[idx]
        path = os.path.join(self.img_dir, img_id)
        rgb = read_rgb(path)
        H0, W0 = rgb.shape[0], rgb.shape[1]
        mask = np.zeros((H0, W0), dtype=np.uint8)
        for rle in self.id2rles.get(img_id, []):
            mask = np.maximum(mask, rle_decode_fortran(rle, (H0, W0)))
        rgb = resize_rgb(rgb, self.size)
        mask = resize_mask(mask, self.size)
        x = torch.from_numpy(rgb).permute(2, 0, 1).contiguous().float() / 255.0
        x = normalize_imagenet(x)
        y = torch.from_numpy(mask).unsqueeze(0).float()
        return x, y, img_id


class AirbusTestDataset(Dataset):
    def __init__(self, img_dir: str, image_ids: List[str], size: int):
        self.img_dir = img_dir
        self.ids = image_ids
        self.size = size

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx: int):
        img_id = self.ids[idx]
        path = os.path.join(self.img_dir, img_id)
        rgb = read_rgb(path)
        rgb = resize_rgb(rgb, self.size)
        x = torch.from_numpy(rgb).permute(2, 0, 1).contiguous().float() / 255.0
        x = normalize_imagenet(x)
        return x, img_id


# -------------------------
# Model (DeepLabV3-ResNet50 binary)
# -------------------------
def build_model(pretrained_backbone: bool = True) -> nn.Module:
    weights = "DEFAULT" if pretrained_backbone else None
    m = torchvision.models.segmentation.deeplabv3_resnet50(weights=weights)
    in_ch = m.classifier[-1].in_channels
    m.classifier[-1] = nn.Conv2d(in_ch, 1, kernel_size=1)
    return m


# -------------------------
# Loss + metrics
# -------------------------
def soft_tversky_from_prob(prob: torch.Tensor, target: torch.Tensor, alpha: float, beta: float, eps: float) -> torch.Tensor:
    # prob/target: (B,1,H,W)
    tp = (prob * target).sum(dim=(1,2,3))
    fp = (prob * (1 - target)).sum(dim=(1,2,3))
    fn = ((1 - prob) * target).sum(dim=(1,2,3))
    return (tp + eps) / (tp + alpha * fp + beta * fn + eps)

def tversky_loss_from_logits(logits: torch.Tensor, target: torch.Tensor, alpha: float, beta: float, eps: float) -> torch.Tensor:
    prob = torch.sigmoid(logits)
    return 1.0 - soft_tversky_from_prob(prob, target, alpha=alpha, beta=beta, eps=eps)

def prf_iou_from_binary(pred: np.ndarray, gt: np.ndarray, eps: float = 1e-8) -> Dict[str, float]:
    tp = float((pred * gt).sum())
    fp = float((pred * (1 - gt)).sum())
    fn = float(((1 - pred) * gt).sum())
    prec = tp / (tp + fp + eps)
    rec = tp / (tp + fn + eps)
    f1 = 2 * prec * rec / (prec + rec + eps)
    f2 = (1 + 4) * prec * rec / (4 * prec + rec + eps)
    inter = float((pred * gt).sum())
    union = float(((pred + gt) > 0).sum())
    iou = (inter + eps) / (union + eps)
    return {"Precision": prec, "Recall": rec, "F1": f1, "F2": f2, "IoU": iou}


# -------------------------
# TTA (4-way flip)
# -------------------------
@torch.no_grad()
def predict_proba_tta(model: nn.Module, x: torch.Tensor) -> torch.Tensor:
    model.eval()
    probs = []

    out = model(x)["out"]
    probs.append(torch.sigmoid(out))

    x_h = torch.flip(x, dims=[3])
    out_h = model(x_h)["out"]
    out_h = torch.flip(out_h, dims=[3])
    probs.append(torch.sigmoid(out_h))

    x_v = torch.flip(x, dims=[2])
    out_v = model(x_v)["out"]
    out_v = torch.flip(out_v, dims=[2])
    probs.append(torch.sigmoid(out_v))

    x_hv = torch.flip(x, dims=[2,3])
    out_hv = model(x_hv)["out"]
    out_hv = torch.flip(out_hv, dims=[2,3])
    probs.append(torch.sigmoid(out_hv))

    return torch.stack(probs, dim=0).mean(dim=0)


# -------------------------
# Postprocess & instances
# -------------------------
def remove_small_components(mask: np.ndarray, min_area: int) -> np.ndarray:
    if mask.sum() == 0 or min_area <= 0:
        return mask
    if _HAS_CV2:
        num, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)
        out = np.zeros_like(mask)
        for i in range(1, num):
            area = stats[i, cv2.CC_STAT_AREA]
            if area >= min_area:
                out[labels == i] = 1
        return out
    if _HAS_SCIPY:
        labels, num = ndi.label(mask)
        out = np.zeros_like(mask)
        for i in range(1, num+1):
            comp = (labels == i)
            if int(comp.sum()) >= min_area:
                out[comp] = 1
        return out
    raise RuntimeError("Need cv2 or scipy for connected components")

def binarize_postprocess(prob: np.ndarray, thr: float, min_area: int) -> np.ndarray:
    m = (prob >= thr).astype(np.uint8)
    m = remove_small_components(m, min_area=min_area)
    return m

def instances_from_mask(mask: np.ndarray, min_area: int) -> List[np.ndarray]:
    if mask.sum() == 0:
        return []
    if _HAS_CV2:
        num, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)
        insts = []
        for i in range(1, num):
            if stats[i, cv2.CC_STAT_AREA] >= min_area:
                insts.append((labels == i).astype(np.uint8))
        return insts
    if _HAS_SCIPY:
        labels, num = ndi.label(mask)
        insts = []
        for i in range(1, num+1):
            comp = (labels == i).astype(np.uint8)
            if int(comp.sum()) >= min_area:
                insts.append(comp)
        return insts
    raise RuntimeError("Need cv2 or scipy for connected components")


# -------------------------
# Threshold tuning (fast histogram) — maximize F2
# -------------------------
@torch.no_grad()
def tune_threshold_f2_hist(
    model: nn.Module,
    val_ds: AirbusTrainDataset,
    device: torch.device,
    batch_eval: int,
    num_workers: int,
    limit_images: int,
    thr_grid_n: int,
    eps: float,
    log_global: DualLogger,
) -> float:
    model.eval()
    n = len(val_ds)
    limit = min(limit_images, n)

    bins = 1000
    hist_pos = np.zeros(bins, dtype=np.float64)
    hist_neg = np.zeros(bins, dtype=np.float64)
    total_pos = 0.0

    loader = DataLoader(val_ds, batch_size=batch_eval, shuffle=False,
                        num_workers=num_workers, pin_memory=True, drop_last=False)

    seen = 0
    for x, y, _ in loader:
        if seen >= limit:
            break
        take = min(x.shape[0], limit - seen)
        x = x[:take].to(device, non_blocking=True)
        yb = y[:take].numpy().astype(np.uint8)
        prob = predict_proba_tta(model, x).detach().cpu().numpy()
        for bi in range(take):
            pr = prob[bi,0].reshape(-1)
            gt = yb[bi,0].reshape(-1)
            pos_vals = pr[gt == 1]
            neg_vals = pr[gt == 0]
            if pos_vals.size > 0:
                hp, _ = np.histogram(pos_vals, bins=bins, range=(0.0, 1.0))
                hist_pos += hp
                total_pos += float(pos_vals.size)
            if neg_vals.size > 0:
                hn, _ = np.histogram(neg_vals, bins=bins, range=(0.0, 1.0))
                hist_neg += hn
        seen += take

    cpos = np.cumsum(hist_pos[::-1])[::-1]
    cneg = np.cumsum(hist_neg[::-1])[::-1]
    eps2 = 1e-8

    thrs = np.linspace(0.0, 1.0, thr_grid_n, dtype=np.float32)
    best_thr = 0.5
    best_f2 = -1.0

    for thr in thrs:
        b = int(min(bins-1, max(0, int(thr * (bins-1)))))
        TP = cpos[b]
        FP = cneg[b]
        FN = max(0.0, total_pos - TP)
        prec = TP / (TP + FP + eps2)
        rec = TP / (TP + FN + eps2)
        f2 = (1 + 4) * prec * rec / (4 * prec + rec + eps2)
        if f2 > best_f2:
            best_f2 = float(f2)
            best_thr = float(thr)

    log_global.log(f"[THR_TUNE] method=hist_f2 limit_images={limit} thr_grid_n={thr_grid_n} "
                   f"best_thr={best_thr:.3f} best_f2={best_f2:.6f}")
    return best_thr


# -------------------------
# Train / Eval
# -------------------------
@dataclass
class TrainConfig:
    data_dir: str
    out_dir: str

    seed: int = 42

    train_n: int = 20000
    val_n: int = 3000
    pseudo_test_n: int = 3000
    pos_frac: float = 0.40

    train_size: int = 256
    infer_size: int = 768

    batch_train: int = 4
    batch_eval: int = 1
    num_workers: int = 4

    lr: float = 3e-4
    weight_decay: float = 1e-4

    max_epochs: int = 50
    patience: int = 6

    use_pos_weight: bool = True

    # tversky
    alpha: float = 0.6  # FP weight (precision side)
    beta: float = 0.4   # FN weight (recall side)

    # threshold tuning
    thr_tune_limit: int = 1200
    thr_grid_n: int = 41
    min_area: int = 50

    # logs
    log_every: int = 50

    # smoothing
    eps: float = 1e-6


def compute_pos_weight(train_ids: List[str], pos_set: set) -> float:
    pos = sum(1 for x in train_ids if x in pos_set)
    neg = len(train_ids) - pos
    if pos <= 0:
        return 1.0
    return float(neg) / float(pos)

def save_json(path: str, obj):
    d = os.path.dirname(path)
    if d:
        os.makedirs(d, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False)

def torch_save(path: str, obj):
    d = os.path.dirname(path)
    if d:
        os.makedirs(d, exist_ok=True)
    torch.save(obj, path)

def torch_load(path: str, device: torch.device):
    return torch.load(path, map_location=device)

def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    opt: torch.optim.Optimizer,
    cfg: TrainConfig,
    device: torch.device,
    log_detail: DualLogger,
    pos_weight_val: float,
) -> Dict[str, float]:
    model.train()
    total_loss = 0.0
    total_bce = 0.0
    total_tversky = 0.0
    n = 0
    t0 = time.time()

    pw_t = torch.tensor([pos_weight_val], device=device, dtype=torch.float32)

    for it, (x, y, img_ids) in enumerate(loader):
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)

        opt.zero_grad(set_to_none=True)
        logits = model(x)["out"]

        # per-sample BCE
        # we compute BCE map to get per-sample mean; apply pos_weight if enabled
        if cfg.use_pos_weight:
            bce_map = F.binary_cross_entropy_with_logits(logits, y, reduction="none", pos_weight=pw_t)
        else:
            bce_map = F.binary_cross_entropy_with_logits(logits, y, reduction="none")
        bce_each = bce_map.mean(dim=(1,2,3))

        # per-sample Tversky loss
        tversky_each = tversky_loss_from_logits(logits, y, alpha=cfg.alpha, beta=cfg.beta, eps=cfg.eps)

        loss_each = bce_each + tversky_each
        loss = loss_each.mean()

        # NaN/Inf detection per-sample
        if torch.isnan(loss).any() or torch.isinf(loss).any():
            for bi in range(loss_each.shape[0]):
                if torch.isnan(loss_each[bi]) or torch.isinf(loss_each[bi]):
                    yt = y[bi].detach().cpu().numpy()
                    pr = torch.sigmoid(logits[bi]).detach().cpu().numpy()
                    log_detail.log(
                        f"[NAN] img_id={img_ids[bi]} loss_each={float(loss_each[bi])} "
                        f"y_sum={float(yt.sum()):.1f} y_minmax=({yt.min():.3f},{yt.max():.3f}) "
                        f"p_minmax=({pr.min():.6f},{pr.max():.6f}) p_mean={pr.mean():.6f}"
                    )
            raise RuntimeError("NaN/Inf loss detected. See run_detail.log")

        loss.backward()
        opt.step()

        total_loss += float(loss.item())
        total_bce += float(bce_each.mean().item())
        total_tversky += float(tversky_each.mean().item())
        n += 1

        if (it % cfg.log_every) == 0:
            dt = time.time() - t0
            log_detail.log(
                f"[ITER] it={it:05d} loss={loss.item():.6f} bce={bce_each.mean().item():.6f} "
                f"tversky_loss={tversky_each.mean().item():.6f} sec={dt:.1f}"
            )
            t0 = time.time()

    return {"loss": total_loss / max(n,1), "bce": total_bce / max(n,1), "tversky_loss": total_tversky / max(n,1)}

@torch.no_grad()
def eval_val_soft_tversky(
    model: nn.Module,
    loader: DataLoader,
    cfg: TrainConfig,
    device: torch.device
) -> float:
    model.eval()
    vals = []
    for x, y, _ in loader:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        prob = predict_proba_tta(model, x)
        tv = soft_tversky_from_prob(prob, y, alpha=cfg.alpha, beta=cfg.beta, eps=cfg.eps)
        vals.append(tv.detach().cpu())
    if not vals:
        return 0.0
    return float(torch.cat(vals).mean().item())

@torch.no_grad()
def eval_pseudo_test_metrics(
    model: nn.Module,
    ds: AirbusTrainDataset,
    cfg: TrainConfig,
    device: torch.device,
    thr: float
) -> Dict[str, float]:
    model.eval()
    loader = DataLoader(ds, batch_size=cfg.batch_eval, shuffle=False,
                        num_workers=cfg.num_workers, pin_memory=True, drop_last=False)
    TP = FP = FN = 0.0
    inter = union = 0.0
    eps2 = 1e-8

    for x, y, _ in loader:
        x = x.to(device, non_blocking=True)
        prob = predict_proba_tta(model, x).detach().cpu().numpy()
        gt = y.numpy().astype(np.uint8)
        for bi in range(prob.shape[0]):
            pr = prob[bi,0]
            g = gt[bi,0]
            pm = binarize_postprocess(pr, thr=thr, min_area=cfg.min_area)
            TP += float((pm * g).sum())
            FP += float((pm * (1 - g)).sum())
            FN += float(((1 - pm) * g).sum())
            inter += float((pm * g).sum())
            union += float(((pm + g) > 0).sum())

    prec = TP / (TP + FP + eps2)
    rec = TP / (TP + FN + eps2)
    f1 = 2 * prec * rec / (prec + rec + eps2)
    f2 = (1 + 4) * prec * rec / (4 * prec + rec + eps2)
    iou = (inter + eps2) / (union + eps2)
    return {"Precision": float(prec), "Recall": float(rec), "F1": float(f1), "F2": float(f2), "IoU": float(iou)}


# -------------------------
# Submission (variable rows)
# -------------------------
def load_sample_submission_ids(sample_csv: str) -> List[str]:
    ids = []
    with open(sample_csv, "r", newline="") as f:
        reader = csv.DictReader(f)
        if reader.fieldnames != ["ImageId", "EncodedPixels"]:
            raise RuntimeError(f"Unexpected sample_submission header: {reader.fieldnames}")
        for row in reader:
            ids.append(row["ImageId"])
    if len(ids) == 0:
        raise RuntimeError("sample_submission_v2.csv is empty")
    return ids

@torch.no_grad()
def make_submission_csv(
    model: nn.Module,
    test_img_dir: str,
    sample_ids: List[str],
    cfg: TrainConfig,
    device: torch.device,
    thr: float,
    out_csv_path: str,
    log_global: DualLogger
) -> Dict[str, float]:
    model.eval()
    ds = AirbusTestDataset(test_img_dir, sample_ids, size=cfg.infer_size)
    loader = DataLoader(ds, batch_size=cfg.batch_eval, shuffle=False,
                        num_workers=cfg.num_workers, pin_memory=True, drop_last=False)

    os.makedirs(os.path.dirname(out_csv_path), exist_ok=True) if os.path.dirname(out_csv_path) else None

    total_rows = 0
    nonempty_rows = 0
    empty_images = 0

    with open(out_csv_path, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["ImageId", "EncodedPixels"])
        for x, ids in loader:
            x = x.to(device, non_blocking=True)
            prob = predict_proba_tta(model, x).detach().cpu().numpy()
            for bi in range(prob.shape[0]):
                img_id = ids[bi]
                pr = prob[bi,0]
                mask = binarize_postprocess(pr, thr=thr, min_area=cfg.min_area)
                insts = instances_from_mask(mask, min_area=cfg.min_area)
                if len(insts) == 0:
                    w.writerow([img_id, ""])
                    total_rows += 1
                    empty_images += 1
                else:
                    for inst in insts:
                        rle = rle_encode_fortran(inst)
                        w.writerow([img_id, rle])
                        total_rows += 1
                        nonempty_rows += 1

    size_bytes = os.path.getsize(out_csv_path)
    sha1 = hashlib.sha1()
    with open(out_csv_path, "rb") as fb:
        sha1.update(fb.read(min(1024*1024, size_bytes)))
    head_sha1 = sha1.hexdigest()

    log_global.log(f"[SUBMIT] out_csv={out_csv_path}")
    log_global.log(f"[SUBMIT] total_rows={total_rows} nonempty_rows={nonempty_rows} empty_images={empty_images}")
    log_global.log(f"[SUBMIT] file_size_bytes={size_bytes} head_sha1_first1MB={head_sha1}")

    return {
        "submission_rows_total": float(total_rows),
        "submission_rows_nonempty": float(nonempty_rows),
        "submission_images_empty": float(empty_images),
        "submission_file_size_bytes": float(size_bytes),
        "submission_head_sha1_first1MB": head_sha1,
    }


# -------------------------
# Main
# -------------------------
def main():
    rle_roundtrip_selftest()

    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, default="/workspace/kaggle_competition/airbus-ship-detection")
    ap.add_argument("--out_dir", type=str, default="/workspace/kaggle_competition/outputs_airbus_ablation_tversky")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--train_n", type=int, default=20000)
    ap.add_argument("--val_n", type=int, default=3000)
    ap.add_argument("--pseudo_test_n", type=int, default=3000)
    ap.add_argument("--pos_frac", type=float, default=0.40)
    ap.add_argument("--train_size", type=int, default=256)
    ap.add_argument("--infer_size", type=int, default=768)
    ap.add_argument("--batch_train", type=int, default=4)
    ap.add_argument("--batch_eval", type=int, default=1)
    ap.add_argument("--num_workers", type=int, default=4)
    ap.add_argument("--lr", type=float, default=3e-4)
    ap.add_argument("--weight_decay", type=float, default=1e-4)
    ap.add_argument("--max_epochs", type=int, default=50)
    ap.add_argument("--patience", type=int, default=6)
    ap.add_argument("--use_pos_weight", type=int, default=1)
    ap.add_argument("--alpha", type=float, default=0.6)
    ap.add_argument("--beta", type=float, default=0.4)
    ap.add_argument("--thr_tune_limit", type=int, default=1200)
    ap.add_argument("--thr_grid_n", type=int, default=41)
    ap.add_argument("--min_area", type=int, default=50)
    ap.add_argument("--log_every", type=int, default=50)
    ap.add_argument("--eps", type=float, default=1e-6)
    args = ap.parse_args()

    cfg = TrainConfig(
        data_dir=args.data_dir,
        out_dir=args.out_dir,
        seed=args.seed,
        train_n=args.train_n,
        val_n=args.val_n,
        pseudo_test_n=args.pseudo_test_n,
        pos_frac=args.pos_frac,
        train_size=args.train_size,
        infer_size=args.infer_size,
        batch_train=args.batch_train,
        batch_eval=args.batch_eval,
        num_workers=args.num_workers,
        lr=args.lr,
        weight_decay=args.weight_decay,
        max_epochs=args.max_epochs,
        patience=args.patience,
        use_pos_weight=bool(args.use_pos_weight),
        alpha=args.alpha,
        beta=args.beta,
        thr_tune_limit=args.thr_tune_limit,
        thr_grid_n=args.thr_grid_n,
        min_area=args.min_area,
        log_every=args.log_every,
        eps=args.eps,
    )

    run_name = f"run__bce_tversky__seed{cfg.seed}__train{cfg.train_n}__val{cfg.val_n}__ptest{cfg.pseudo_test_n}__pos{int(cfg.pos_frac*100):02d}__a{cfg.alpha:.2f}b{cfg.beta:.2f}"
    run_dir = os.path.join(cfg.out_dir, run_name)
    os.makedirs(run_dir, exist_ok=True)

    log_global = DualLogger(os.path.join(run_dir, "run_global.log"), also_stdout=True)
    log_detail = DualLogger(os.path.join(run_dir, "run_detail.log"), also_stdout=False)

    log_global.log("==== START intern-winter Ablation: BCE+Tversky ====")
    log_global.log(f"data_dir={cfg.data_dir}")
    log_global.log(f"run_dir={run_dir}")
    log_global.log(f"config={json.dumps(asdict(cfg), ensure_ascii=False)}")
    log_global.log(f"cv2_available={_HAS_CV2} scipy_available={_HAS_SCIPY}")

    seed_everything(cfg.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    log_global.log(f"device={device}")

    train_img_dir = os.path.join(cfg.data_dir, "train_v2")
    test_img_dir  = os.path.join(cfg.data_dir, "test_v2")
    csv_path      = os.path.join(cfg.data_dir, "train_ship_segmentations_v2.csv")
    sample_csv    = os.path.join(cfg.data_dir, "sample_submission_v2.csv")

    if not os.path.exists(train_img_dir):
        raise RuntimeError(f"train_v2 not found: {train_img_dir}")
    if not os.path.exists(csv_path):
        raise RuntimeError(f"train_ship_segmentations_v2.csv not found: {csv_path}")

    id2rles = load_id2rles(csv_path)
    pos_set = set(id2rles.keys())
    all_ids = list_image_ids(train_img_dir)

    split_cfg = SplitConfig(seed=cfg.seed, train_n=cfg.train_n, val_n=cfg.val_n, pseudo_test_n=cfg.pseudo_test_n, pos_frac=cfg.pos_frac)
    splits = stratified_disjoint_split(all_ids, pos_set, split_cfg)

    save_json(os.path.join(run_dir, "splits.json"), {
        "seed": cfg.seed,
        "train_n": cfg.train_n, "val_n": cfg.val_n, "pseudo_test_n": cfg.pseudo_test_n,
        "pos_frac": cfg.pos_frac,
        "train": splits["train"],
        "val": splits["val"],
        "pseudo_test": splits["pseudo_test"],
    })

    pos_weight_val = compute_pos_weight(splits["train"], pos_set) if cfg.use_pos_weight else 1.0
    log_global.log(f"pos_weight_for_BCE={pos_weight_val:.6f} use_pos_weight={cfg.use_pos_weight}")

    ds_train = AirbusTrainDataset(train_img_dir, splits["train"], id2rles, size=cfg.train_size)
    ds_val   = AirbusTrainDataset(train_img_dir, splits["val"], id2rles, size=cfg.infer_size)
    ds_ptest = AirbusTrainDataset(train_img_dir, splits["pseudo_test"], id2rles, size=cfg.infer_size)

    dl_train = DataLoader(ds_train, batch_size=cfg.batch_train, shuffle=True,
                          num_workers=cfg.num_workers, pin_memory=True, drop_last=False)
    dl_val = DataLoader(ds_val, batch_size=cfg.batch_eval, shuffle=False,
                        num_workers=cfg.num_workers, pin_memory=True, drop_last=False)

    model = build_model(pretrained_backbone=True).to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    best_epoch = -1
    best_val_soft_tversky = -1.0
    best_ckpt_path = os.path.join(run_dir, "best_model.pt")
    bad_epochs = 0

    log_global.log("[INFO] Training begins... (best by VAL soft-Tversky, TTA mandatory)")

    for epoch in range(1, cfg.max_epochs + 1):
        t_epoch0 = time.time()

        train_stats = train_one_epoch(model, dl_train, opt, cfg, device, log_detail, pos_weight_val=pos_weight_val)
        val_soft_tv = eval_val_soft_tversky(model, dl_val, cfg, device)

        improved = val_soft_tv > best_val_soft_tversky + 1e-12
        if improved:
            best_val_soft_tversky = val_soft_tv
            best_epoch = epoch
            bad_epochs = 0
            torch_save(best_ckpt_path, {"model": model.state_dict(), "epoch": epoch, "val_soft_tversky": val_soft_tv})
        else:
            bad_epochs += 1

        dt = time.time() - t_epoch0
        log_global.log(
            f"[EPOCH] {epoch:03d}/{cfg.max_epochs} "
            f"train_loss={train_stats['loss']:.6f} train_bce={train_stats['bce']:.6f} train_tversky_loss={train_stats['tversky_loss']:.6f} "
            f"val_soft_tversky={val_soft_tv:.6f} best_soft_tversky={best_val_soft_tversky:.6f} best_epoch={best_epoch} "
            f"bad_epochs={bad_epochs}/{cfg.patience} sec={dt:.1f}"
        )

        if bad_epochs >= cfg.patience:
            log_global.log(f"[EARLY_STOP] triggered at epoch={epoch}, best_epoch={best_epoch}, best_val_soft_tversky={best_val_soft_tversky:.6f}")
            break

    ck = torch_load(best_ckpt_path, device)
    model.load_state_dict(ck["model"])
    log_global.log(f"[BEST] loaded best ckpt epoch={ck.get('epoch')} val_soft_tversky={ck.get('val_soft_tversky')}")

    # Threshold tuning on VAL (F2, TTA mandatory)
    best_thr = tune_threshold_f2_hist(
        model=model, val_ds=ds_val, device=device,
        batch_eval=cfg.batch_eval, num_workers=cfg.num_workers,
        limit_images=cfg.thr_tune_limit, thr_grid_n=cfg.thr_grid_n,
        eps=cfg.eps, log_global=log_global
    )

    # Pseudo-test evaluation
    ptest_metrics = eval_pseudo_test_metrics(model, ds_ptest, cfg, device, thr=best_thr)
    log_global.log(f"[PSEUDO_TEST] metrics={json.dumps(ptest_metrics)} thr={best_thr:.3f} min_area={cfg.min_area}")

    # Kaggle test -> submission
    if os.path.exists(sample_csv) and os.path.exists(test_img_dir):
        sample_ids = load_sample_submission_ids(sample_csv)
        submission_path = os.path.join(run_dir, "submission.csv")
        submit_stats = make_submission_csv(
            model=model, test_img_dir=test_img_dir, sample_ids=sample_ids,
            cfg=cfg, device=device, thr=best_thr, out_csv_path=submission_path, log_global=log_global
        )
    else:
        submission_path = ""
        submit_stats = {}
        log_global.log("[WARN] sample_submission_v2.csv or test_v2 not found; skip submission generation.")

    summary = {
        "run_name": run_name,
        "run_dir": run_dir,
        "condition": "BCE+Tversky",
        "tversky_alpha_beta": {"alpha": cfg.alpha, "beta": cfg.beta},
        "config": asdict(cfg),
        "best_epoch": int(ck.get("epoch", -1)),
        "best_val_soft_tversky": float(ck.get("val_soft_tversky", -1.0)),
        "thr_best_f2": float(best_thr),
        "pseudo_test_metrics": ptest_metrics,
        "submission_path": submission_path,
        "submission_stats": submit_stats,
    }
    save_json(os.path.join(run_dir, "summary.json"), summary)

    log_global.log("[DONE] wrote summary.json (and submission.csv if available)")
    log_global.close()
    log_detail.close()


if __name__ == "__main__":
    main()
